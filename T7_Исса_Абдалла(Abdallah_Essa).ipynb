{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOy5PBRKxOekPMgs7U+imzJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbdAllAh950/Algorithms/blob/main/T7_%D0%98%D1%81%D1%81%D0%B0_%D0%90%D0%B1%D0%B4%D0%B0%D0%BB%D0%BB%D0%B0(Abdallah_Essa).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install beautifulsoup4 bitarray\n",
        "\n",
        "# Import libraries\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter, defaultdict\n",
        "import heapq\n",
        "from bitarray import bitarray"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FD4gzKOaZdZ2",
        "outputId": "926b5f49-4aad-41fa-d873-aac12e5fade4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Collecting bitarray\n",
            "  Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (32 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n",
            "Downloading bitarray-3.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.3/278.3 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitarray\n",
            "Successfully installed bitarray-3.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Fetch and prepare text\n",
        "def fetch_text_from_url(url):\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        poem_div = soup.find(\"div\", class_=\"poem-text\")\n",
        "        if poem_div:\n",
        "            return poem_div.get_text(separator=\" \").strip()\n",
        "        else:\n",
        "            return \"Fallback text: By the shores of Gitche Gumee, By the shining Big-Sea-Water,\" \\\n",
        "                   \" Stood the wigwam of Nokomis, Daughter of the Moon, Nokomis.\"\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching text: {e}\")\n",
        "        return \"Fallback text: By the shores of Gitche Gumee, By the shining Big-Sea-Water,\" \\\n",
        "               \" Stood the wigwam of Nokomis, Daughter of the Moon, Nokomis.\""
      ],
      "metadata": {
        "id": "AdlajoQtZfu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# URL of the poem\n",
        "url = \"https://www.hwlongfellow.org/poems_poem.php?pid=62\"\n",
        "text = fetch_text_from_url(url)"
      ],
      "metadata": {
        "id": "SGxLxBloZixL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Huffman Encoding\n",
        "def build_huffman_tree(freq_table):\n",
        "    heap = [[weight, [symbol, \"\"]] for symbol, weight in freq_table.items()]\n",
        "    heapq.heapify(heap)\n",
        "    while len(heap) > 1:\n",
        "        lo = heapq.heappop(heap)\n",
        "        hi = heapq.heappop(heap)\n",
        "        for pair in lo[1:]:\n",
        "            pair[1] = '0' + pair[1]\n",
        "        for pair in hi[1:]:\n",
        "            pair[1] = '1' + pair[1]\n",
        "        heapq.heappush(heap, [lo[0] + hi[0]] + lo[1:] + hi[1:])\n",
        "    return sorted(heapq.heappop(heap)[1:], key=lambda p: (len(p[-1]), p))\n",
        "\n",
        "def huffman_encode(text, huffman_code):\n",
        "    encoded = bitarray()\n",
        "    for char in text:\n",
        "        encoded.extend(huffman_code[char])\n",
        "    return encoded\n",
        "\n",
        "freq_table = Counter(text)\n",
        "huffman_tree = build_huffman_tree(freq_table)\n",
        "huffman_code = {symbol: bitarray(code) for symbol, code in huffman_tree}\n",
        "encoded_text_huffman = huffman_encode(text, huffman_code)\n",
        "original_size = len(text) * 8  # ASCII has 8 bits per character\n",
        "compressed_size_huffman = len(encoded_text_huffman)\n",
        "compression_ratio_huffman = original_size / compressed_size_huffman"
      ],
      "metadata": {
        "id": "FROI5APIZm2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: LZW Compression\n",
        "def lzw_compress(text):\n",
        "    dictionary = {chr(i): i for i in range(256)}\n",
        "    next_code = 256\n",
        "    current = \"\"\n",
        "    compressed = []\n",
        "    for char in text:\n",
        "        combined = current + char\n",
        "        if combined in dictionary:\n",
        "            current = combined\n",
        "        else:\n",
        "            compressed.append(dictionary[current])\n",
        "            dictionary[combined] = next_code\n",
        "            next_code += 1\n",
        "            current = char\n",
        "    if current:\n",
        "        compressed.append(dictionary[current])\n",
        "    return compressed, dictionary\n",
        "\n",
        "compressed_data_lzw, lzw_dict = lzw_compress(text)\n",
        "compressed_size_lzw = len(compressed_data_lzw) * 16  # Assuming 16-bit codes\n",
        "compression_ratio_lzw = original_size / compressed_size_lzw"
      ],
      "metadata": {
        "id": "AUt6Da71ZriE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Country Name Embedding (Explanation)\n",
        "embedding_explanation = \"\"\"\n",
        "To embed a country name (e.g., \"France\") in a dataset for a machine learning task:\n",
        "1. Replace random words in the dataset with country names from a list.\n",
        "2. Use embeddings for natural language processing tasks (e.g., Named Entity Recognition).\n",
        "3. Example: Replace \"X is a beautiful place\" with \"France is a beautiful place.\"\n",
        "This creates a labeled dataset for models to learn recognizing geographic entities.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "mPP5Q348ZtPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Display Results\n",
        "print(f\"Original Size (bits): {original_size}\")\n",
        "print(f\"Huffman Compressed Size (bits): {compressed_size_huffman}\")\n",
        "print(f\"Huffman Compression Ratio: {compression_ratio_huffman:.2f}\")\n",
        "print(f\"LZW Compressed Size (bits): {compressed_size_lzw}\")\n",
        "print(f\"LZW Compression Ratio: {compression_ratio_lzw:.2f}\")\n",
        "print(f\"Huffman Code Table Size: {len(huffman_code)}\")\n",
        "print(f\"LZW Dictionary Size: {len(lzw_dict)}\")\n",
        "print(\"\\nCountry Name Embedding Explanation:\")\n",
        "print(embedding_explanation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1mHlIa_Zugb",
        "outputId": "83b56912-2aef-4b80-e63a-e1c0c8bb3f63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Size (bits): 1080\n",
            "Huffman Compressed Size (bits): 613\n",
            "Huffman Compression Ratio: 1.76\n",
            "LZW Compressed Size (bits): 1664\n",
            "LZW Compression Ratio: 0.65\n",
            "Huffman Code Table Size: 34\n",
            "LZW Dictionary Size: 359\n",
            "\n",
            "Country Name Embedding Explanation:\n",
            "\n",
            "To embed a country name (e.g., \"France\") in a dataset for a machine learning task:\n",
            "1. Replace random words in the dataset with country names from a list.\n",
            "2. Use embeddings for natural language processing tasks (e.g., Named Entity Recognition).\n",
            "3. Example: Replace \"X is a beautiful place\" with \"France is a beautiful place.\"\n",
            "This creates a labeled dataset for models to learn recognizing geographic entities.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}